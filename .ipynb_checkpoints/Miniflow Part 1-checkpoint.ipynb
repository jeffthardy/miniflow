{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MiniFlow Deeper Dive\n",
    "\n",
    "This Notebook is to take a deeper look at the MiniFlow Neural Network framework created in the Udacity Deep Learning Nanodegree Foundations.  The [MiniFlow Class](miniflow.py) was created during the Udacity lessons and the copy included here is from the solution set.\n",
    "\n",
    "There are a few things I would like to explore:\n",
    "1. Visualizing the tutorial test case in a Jupyter Notebook\n",
    "2. Expanding the Network to a slightly more complicated configuration\n",
    "3. Running this code on my ARM/FPGA (Pynq) Development Board for performance comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Basic Configuration\n",
    "\n",
    "The following is a replication of the base example given in the course lesson, but broken out in a more detailed way for viewing in this Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-0ec944622d4b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mminiflow\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "# Bring in the required libraries\n",
    "\n",
    "# We use numpy for creating tensors (N-dimensional Arrays)\n",
    "import numpy as np\n",
    "\n",
    "# Our test dataset will come from the scikit-learn data set of Boston Housing\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "# To perform Stochastic Gradient Decent use shuffle/resample on the base dataset\n",
    "from sklearn.utils import shuffle, resample\n",
    "\n",
    "# Miniflow is a custom set of classes derived from the Node base class\n",
    "from miniflow import *\n",
    "\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next, we load in our data set with both features and targets based on those features.  We normalize the data because it is much easier to deal with values between 1 and 0 when picking hyperparameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = load_boston()\n",
    "X_ = data['data']\n",
    "y_ = data['target']\n",
    "\n",
    "# Normalize data\n",
    "X_ = (X_ - np.mean(X_, axis=0)) / np.std(X_, axis=0)\n",
    "\n",
    "# Create holder for weights and bias values\n",
    "n_features = X_.shape[1]\n",
    "n_hidden = 10\n",
    "W1_ = np.random.randn(n_features, n_hidden)\n",
    "b1_ = np.zeros(n_hidden)\n",
    "W2_ = np.random.randn(n_hidden, 1)\n",
    "b2_ = np.zeros(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we configure the Neural Network using the MiniFlow classes to create a simple network as shown in the following image:\n",
    "\n",
    "<img src=two-layer-graph.png>\n",
    " [Image: Udacity]\n",
    "\n",
    "Every node in this image is a class with a member function called forward and backwards which supports forward and backward propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Neural network configuration\n",
    "X, y = Input(), Input()\n",
    "W1, b1 = Input(), Input()\n",
    "W2, b2 = Input(), Input()\n",
    "\n",
    "l1 = Linear(X, W1, b1)\n",
    "s1 = Sigmoid(l1)\n",
    "l2 = Linear(s1, W2, b2)\n",
    "cost = MSE(y, l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid calculation dependency problems we will sort the network into the a safe order of operations using the [Kahn Algorithm](https://en.wikipedia.org/wiki/Topological_sorting#Kahn.27s_algorithm).  An example of what this does is in the following image where some parallel operations are reordered to ensure all dependent data is calculated first.\n",
    "\n",
    "<img src=topological-sort.jpeg> [Image: Udacity]\n",
    "\n",
    "While this is useful for a serial processing system I'm curious how to handle this in a device like an FPGA which is capable of doing truly parallel processing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sort processing and initialize with starting values\n",
    "feed_dict = {\n",
    "    X: X_,\n",
    "    y: y_,\n",
    "    W1: W1_,\n",
    "    b1: b1_,\n",
    "    W2: W2_,\n",
    "    b2: b2_\n",
    "}\n",
    "graph = topological_sort(feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a configured network and can begin the processing interations using a forward pass to get an initial classification, then backwards propagation of the cost/loss of the prediction and also calculating a gradient to determine which direction we need to nudge the system to get a lower cost in the next prediction.  This process is repeated, using a new set of data each time by taking a random subset of the test data, until we reach the configured number of **epochs**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of examples = 506\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "\n",
    "# Total number of examples\n",
    "m = X_.shape[0]\n",
    "batch_size = 11\n",
    "steps_per_epoch = m // batch_size\n",
    "trainables = [W1, b1, W2, b2]\n",
    "\n",
    "print(\"Total number of examples = {}\".format(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 5.863\n",
      "Epoch: 2, Loss: 8.013\n",
      "Epoch: 3, Loss: 7.386\n",
      "Epoch: 4, Loss: 5.355\n",
      "Epoch: 5, Loss: 6.573\n",
      "Epoch: 6, Loss: 6.622\n",
      "Epoch: 7, Loss: 6.356\n",
      "Epoch: 8, Loss: 6.781\n",
      "Epoch: 9, Loss: 6.026\n",
      "Epoch: 10, Loss: 5.317\n",
      "Epoch: 11, Loss: 6.647\n",
      "Epoch: 12, Loss: 6.730\n",
      "Epoch: 13, Loss: 7.484\n",
      "Epoch: 14, Loss: 6.248\n",
      "Epoch: 15, Loss: 5.831\n",
      "Epoch: 16, Loss: 5.402\n",
      "Epoch: 17, Loss: 5.963\n",
      "Epoch: 18, Loss: 5.847\n",
      "Epoch: 19, Loss: 6.338\n",
      "Epoch: 20, Loss: 5.947\n",
      "Epoch: 21, Loss: 6.418\n",
      "Epoch: 22, Loss: 5.669\n",
      "Epoch: 23, Loss: 5.870\n",
      "Epoch: 24, Loss: 4.984\n",
      "Epoch: 25, Loss: 5.605\n",
      "Epoch: 26, Loss: 5.540\n",
      "Epoch: 27, Loss: 5.102\n",
      "Epoch: 28, Loss: 5.557\n",
      "Epoch: 29, Loss: 6.493\n",
      "Epoch: 30, Loss: 5.679\n",
      "Epoch: 31, Loss: 6.558\n",
      "Epoch: 32, Loss: 5.666\n",
      "Epoch: 33, Loss: 5.986\n",
      "Epoch: 34, Loss: 5.980\n",
      "Epoch: 35, Loss: 6.322\n",
      "Epoch: 36, Loss: 6.671\n",
      "Epoch: 37, Loss: 5.681\n",
      "Epoch: 38, Loss: 5.365\n",
      "Epoch: 39, Loss: 5.333\n",
      "Epoch: 40, Loss: 6.722\n",
      "Epoch: 41, Loss: 5.662\n",
      "Epoch: 42, Loss: 6.183\n",
      "Epoch: 43, Loss: 5.841\n",
      "Epoch: 44, Loss: 5.899\n",
      "Epoch: 45, Loss: 4.955\n",
      "Epoch: 46, Loss: 6.633\n",
      "Epoch: 47, Loss: 5.296\n",
      "Epoch: 48, Loss: 4.715\n",
      "Epoch: 49, Loss: 5.326\n",
      "Epoch: 50, Loss: 5.641\n",
      "Epoch: 51, Loss: 6.050\n",
      "Epoch: 52, Loss: 6.106\n",
      "Epoch: 53, Loss: 6.241\n",
      "Epoch: 54, Loss: 6.108\n",
      "Epoch: 55, Loss: 6.148\n",
      "Epoch: 56, Loss: 4.866\n",
      "Epoch: 57, Loss: 6.078\n",
      "Epoch: 58, Loss: 4.879\n",
      "Epoch: 59, Loss: 4.370\n",
      "Epoch: 60, Loss: 6.409\n",
      "Epoch: 61, Loss: 5.354\n",
      "Epoch: 62, Loss: 4.882\n",
      "Epoch: 63, Loss: 4.995\n",
      "Epoch: 64, Loss: 5.828\n",
      "Epoch: 65, Loss: 6.954\n",
      "Epoch: 66, Loss: 5.437\n",
      "Epoch: 67, Loss: 4.550\n",
      "Epoch: 68, Loss: 6.032\n",
      "Epoch: 69, Loss: 4.886\n",
      "Epoch: 70, Loss: 5.497\n",
      "Epoch: 71, Loss: 5.110\n",
      "Epoch: 72, Loss: 4.782\n",
      "Epoch: 73, Loss: 5.213\n",
      "Epoch: 74, Loss: 6.169\n",
      "Epoch: 75, Loss: 5.892\n",
      "Epoch: 76, Loss: 5.683\n",
      "Epoch: 77, Loss: 5.253\n",
      "Epoch: 78, Loss: 5.467\n",
      "Epoch: 79, Loss: 5.647\n",
      "Epoch: 80, Loss: 5.803\n",
      "Epoch: 81, Loss: 4.733\n",
      "Epoch: 82, Loss: 5.262\n",
      "Epoch: 83, Loss: 4.859\n",
      "Epoch: 84, Loss: 4.894\n",
      "Epoch: 85, Loss: 4.610\n",
      "Epoch: 86, Loss: 5.296\n",
      "Epoch: 87, Loss: 5.194\n",
      "Epoch: 88, Loss: 5.545\n",
      "Epoch: 89, Loss: 5.298\n",
      "Epoch: 90, Loss: 4.404\n",
      "Epoch: 91, Loss: 4.923\n",
      "Epoch: 92, Loss: 4.403\n",
      "Epoch: 93, Loss: 6.478\n",
      "Epoch: 94, Loss: 5.253\n",
      "Epoch: 95, Loss: 4.487\n",
      "Epoch: 96, Loss: 6.234\n",
      "Epoch: 97, Loss: 4.685\n",
      "Epoch: 98, Loss: 4.930\n",
      "Epoch: 99, Loss: 5.780\n",
      "Epoch: 100, Loss: 5.603\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i in range(epochs):\n",
    "    loss = 0\n",
    "    for j in range(steps_per_epoch):\n",
    "        # Step 1\n",
    "        # Randomly sample a batch of examples\n",
    "        X_batch, y_batch = resample(X_, y_, n_samples=batch_size)\n",
    "\n",
    "        # Reset value of X and y Inputs\n",
    "        X.value = X_batch\n",
    "        y.value = y_batch\n",
    "\n",
    "        # Step 2\n",
    "        forward_and_backward(graph)\n",
    "\n",
    "        # Step 3\n",
    "        sgd_update(trainables)\n",
    "\n",
    "        loss += graph[-1].value\n",
    "\n",
    "    print(\"Epoch: {}, Loss: {:.3f}\".format(i+1, loss/steps_per_epoch))\n",
    "    results.append(loss/steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5.8633338738441605,\n",
       " 8.0127587504606446,\n",
       " 7.3860154403587677,\n",
       " 5.3550259367467392,\n",
       " 6.5725783089335534,\n",
       " 6.6221265556431197,\n",
       " 6.355948425303767,\n",
       " 6.7811609796136807,\n",
       " 6.0256527470185253,\n",
       " 5.316911297566139,\n",
       " 6.6471912973057217,\n",
       " 6.7296015207900748,\n",
       " 7.4840896609281717,\n",
       " 6.2479559292802387,\n",
       " 5.8311490148196006,\n",
       " 5.4015216836683502,\n",
       " 5.9631413260991994,\n",
       " 5.8473423544507268,\n",
       " 6.3379864927554221,\n",
       " 5.9473973039855945,\n",
       " 6.4176245195767789,\n",
       " 5.6686978421910981,\n",
       " 5.8701607165997736,\n",
       " 4.9839653352375919,\n",
       " 5.6053872411264809,\n",
       " 5.5397588494680647,\n",
       " 5.1021012609620318,\n",
       " 5.5570191761894492,\n",
       " 6.4927060692747673,\n",
       " 5.6787618231901371,\n",
       " 6.5580451464187357,\n",
       " 5.6656473383607251,\n",
       " 5.9864270487410236,\n",
       " 5.98013097696073,\n",
       " 6.3219846813766294,\n",
       " 6.6706274394943668,\n",
       " 5.6814106910633715,\n",
       " 5.3653137329486356,\n",
       " 5.3333283287709632,\n",
       " 6.7217178926359784,\n",
       " 5.6616738049904338,\n",
       " 6.1828854653243663,\n",
       " 5.8410299732710422,\n",
       " 5.8989012599394899,\n",
       " 4.954629595510383,\n",
       " 6.6333176128720774,\n",
       " 5.2963742664783391,\n",
       " 4.7145388783023705,\n",
       " 5.3255084585537782,\n",
       " 5.6410714638202686,\n",
       " 6.0503557476167176,\n",
       " 6.10633778785287,\n",
       " 6.2405055154117131,\n",
       " 6.1079198460594908,\n",
       " 6.1484211431176741,\n",
       " 4.8657164686702785,\n",
       " 6.0778526996558009,\n",
       " 4.8793961035072044,\n",
       " 4.370454415262631,\n",
       " 6.4092666979086408,\n",
       " 5.3536998301596261,\n",
       " 4.8818995107133469,\n",
       " 4.9947676945841168,\n",
       " 5.8277169673083993,\n",
       " 6.9543869143325532,\n",
       " 5.4370978721466088,\n",
       " 4.5495297476813219,\n",
       " 6.0322873892931517,\n",
       " 4.8864183099622966,\n",
       " 5.4971561382020466,\n",
       " 5.1097212716742435,\n",
       " 4.7815843497016575,\n",
       " 5.2133653939349642,\n",
       " 6.1685874718243152,\n",
       " 5.8924223043458328,\n",
       " 5.6833417793164234,\n",
       " 5.2531072681972493,\n",
       " 5.4673447668667503,\n",
       " 5.646537928308633,\n",
       " 5.803184146130886,\n",
       " 4.7333036931788719,\n",
       " 5.2623455938671917,\n",
       " 4.8588581958324735,\n",
       " 4.8936928027766475,\n",
       " 4.6098214953193386,\n",
       " 5.2955550343288964,\n",
       " 5.1944665534299341,\n",
       " 5.544907093920207,\n",
       " 5.2977249732511655,\n",
       " 4.4044038985575726,\n",
       " 4.9225393290487736,\n",
       " 4.4029530995500439,\n",
       " 6.478314816548596,\n",
       " 5.2526045263193302,\n",
       " 4.4874495872613034,\n",
       " 6.2341815275778449,\n",
       " 4.6846559376716668,\n",
       " 4.9304889535195011,\n",
       " 5.7795373756217971,\n",
       " 5.6028833309095782]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
